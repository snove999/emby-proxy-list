import os
import re
import csv
import json
import time
import asyncio
import socket
from pathlib import Path
from datetime import datetime, timezone
from typing import Set, Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from urllib.parse import urlparse
import logging

import requests
from bs4 import BeautifulSoup

# ============================================================
# é…ç½®
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-7s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# ç¯å¢ƒå˜é‡é…ç½®
SKIP_VALIDATION = os.environ.get('SKIP_VALIDATION', 'false').lower() == 'true'
VALIDATION_TIMEOUT = float(os.environ.get('VALIDATION_TIMEOUT', '3'))
VALIDATION_CONCURRENCY = int(os.environ.get('VALIDATION_CONCURRENCY', '100'))
OUTPUT_DIR = os.environ.get('OUTPUT_DIR', 'output')

# è„šæœ¬ç›®å½•
SCRIPT_DIR = Path(__file__).parent.resolve()

# ============================================================
# æ•°æ®æºé…ç½®
# ============================================================

# è¿œç¨‹æ•°æ®æº
REMOTE_SOURCES = [
    {
        "name": "ipTop10.html",
        "url": "https://raw.githubusercontent.com/chnbsdan/cf-speed-dns/refs/heads/main/ipTop10.html",
        "type": "html",
        "category": "cloudflare"
    },
    {
        "name": "edgetunnel-output",
        "url": "https://raw.githubusercontent.com/chnbsdan/edgetunnel3/refs/heads/main/output.txt",
        "type": "text",
        "category": "cloudflare"
    },
    {
        "name": "bestproxy",
        "url": "https://ipdb.api.030101.xyz/?type=bestproxy&country=true",
        "type": "text",
        "category": "proxy"
    },
    {
        "name": "bestcf",
        "url": "https://ipdb.api.030101.xyz/?type=bestcf",
        "type": "text",
        "category": "proxy"
    },
    {
        "name": "socks5-proxy",
        "url": "https://raw.githubusercontent.com/chnbsdan/free-proxy-list/refs/heads/main/proxy.txt",
        "type": "socks5_rich",
        "category": "socks5"
    }
]

# æœ¬åœ°æ•°æ®æºï¼ˆç›¸å¯¹äº scripts ç›®å½•ï¼‰
LOCAL_SOURCES = [
    {
        "name": "resultsUS",
        "file": "resultsUS.txt",
        "type": "text",
        "category": "local-US",
        "region_hint": "ç¾å›½"
    },
    {
        "name": "ResultsCN",
        "file": "ResultsCN.txt",
        "type": "text",
        "category": "local-CN",
        "region_hint": "ä¸­å›½"
    },
    {
        "name": "ResultsAHT",
        "file": "ResultsAHT.txt",
        "type": "text",
        "category": "local-AHT",
        "region_hint": "éŸ©å›½"
    },
    {
        "name": "results",
        "file": "results.txt",
        "type": "text",
        "category": "local-general",
        "region_hint": "é€šç”¨"
    }
]


# ============================================================
# æ•°æ®ç»“æ„
# ============================================================

@dataclass
class IPEntry:
    """IP æ¡ç›®æ•°æ®ç»“æ„"""
    ip: str
    port: Optional[int] = None
    
    # æ¥æºä¿¡æ¯
    source: str = ""
    category: str = ""
    
    # åœ°ç†ä¿¡æ¯
    country: str = ""
    region: str = ""
    city: str = ""
    isp: str = ""
    
    # ç½‘ç»œç±»å‹: æœºæˆ¿ / å®¶å®½ / unknown
    net_type: str = ""
    
    # éªŒè¯ç»“æœ
    is_valid: Optional[bool] = None
    latency_ms: Optional[float] = None
    validation_error: str = ""
    
    @property
    def address(self) -> str:
        if self.port:
            return f"{self.ip}:{self.port}"
        return self.ip
    
    @property
    def location(self) -> str:
        parts = []
        if self.country:
            parts.append(self.country)
        if self.city:
            parts.append(self.city)
        elif self.region:
            parts.append(self.region)
        return " ".join(parts) if parts else ""
    
    @property
    def net_type_en(self) -> str:
        mapping = {"æœºæˆ¿": "datacenter", "å®¶å®½": "residential"}
        return mapping.get(self.net_type, "unknown")
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "address": self.address,
            "ip": self.ip,
            "port": self.port,
            "source": self.source,
            "category": self.category,
            "country": self.country,
            "region": self.region,
            "city": self.city,
            "isp": self.isp,
            "net_type": self.net_type,
            "net_type_en": self.net_type_en,
            "location": self.location,
            "is_valid": self.is_valid,
            "latency_ms": self.latency_ms,
            "validation_error": self.validation_error
        }


# ============================================================
# æ­£åˆ™è¡¨è¾¾å¼
# ============================================================

IPV4_PATTERN = r'(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)'

# ä»£ç† URL: protocol://IP:PORT
PROXY_URL_PATTERN = re.compile(
    r'(?:socks[45]?|https?|ss|ssr|vmess|trojan)://'
    r'(?:[^:@\s]+:[^:@\s]+@)?'
    rf'({IPV4_PATTERN}):(\d{{1,5}})',
    re.IGNORECASE
)

# å¯Œä¿¡æ¯ SOCKS5: socks5://IP:PORT [[ç±»å‹] å›½å®¶ çœ åŸå¸‚ [ISP]]
SOCKS5_RICH_PATTERN = re.compile(
    rf'socks[45]?://({IPV4_PATTERN}):(\d{{1,5}})'
    r'\s*'
    r'\[\[([^\]]*)\]\s*'
    r'([^\[]*?)'
    r'\[([^\]]*)\]\]',
    re.IGNORECASE
)

# IP:PORT æˆ– IP#PORT
LOOSE_IP_PORT_PATTERN = re.compile(rf'\b({IPV4_PATTERN})[:#](\d{{1,5}})\b')

# çº¯ IP
PURE_IP_PATTERN = re.compile(rf'\b({IPV4_PATTERN})\b')


# ============================================================
# ç½‘ç»œå·¥å…·
# ============================================================

def fetch_url(url: str, timeout: int = 30, retries: int = 3) -> str:
    """è·å– URL å†…å®¹"""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            logger.warning(f"Attempt {attempt + 1}/{retries} failed: {e}")
            if attempt < retries - 1:
                time.sleep(1)
    return ""


def read_local_file(filepath: Path) -> str:
    """è¯»å–æœ¬åœ°æ–‡ä»¶"""
    try:
        if filepath.exists():
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        else:
            logger.warning(f"File not found: {filepath}")
            return ""
    except Exception as e:
        logger.error(f"Error reading {filepath}: {e}")
        return ""


async def async_tcp_ping(ip: str, port: int, timeout: float = 3.0) -> Tuple[bool, Optional[float], str]:
    """å¼‚æ­¥ TCP æµ‹è¯•"""
    try:
        start = time.time()
        _, writer = await asyncio.wait_for(
            asyncio.open_connection(ip, port),
            timeout=timeout
        )
        elapsed = (time.time() - start) * 1000
        writer.close()
        await writer.wait_closed()
        return True, round(elapsed, 2), ""
    except asyncio.TimeoutError:
        return False, None, "Timeout"
    except ConnectionRefusedError:
        return False, None, "Refused"
    except OSError as e:
        return False, None, str(e)[:20]
    except Exception as e:
        return False, None, str(e)[:20]


async def validate_entries_async(
    entries: List[IPEntry],
    timeout: float = 3.0,
    concurrency: int = 100
) -> None:
    """æ‰¹é‡å¼‚æ­¥éªŒè¯ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰"""
    semaphore = asyncio.Semaphore(concurrency)
    completed = 0
    total = len(entries)
    start_time = time.time()
    
    async def validate_one(entry: IPEntry):
        nonlocal completed
        async with semaphore:
            if entry.port:
                success, latency, error = await async_tcp_ping(entry.ip, entry.port, timeout)
            else:
                # æ— ç«¯å£æ—¶æµ‹è¯•å¸¸ç”¨ç«¯å£
                success, latency, error = False, None, "No port"
                for test_port in [443, 80, 8080, 1080]:
                    success, latency, error = await async_tcp_ping(entry.ip, test_port, timeout / 4)
                    if success:
                        entry.port = test_port  # è®°å½•æœ‰æ•ˆç«¯å£
                        break
            
            entry.is_valid = success
            entry.latency_ms = latency
            entry.validation_error = error
            
            completed += 1
            if completed % 100 == 0 or completed == total:
                elapsed = time.time() - start_time
                rate = completed / elapsed if elapsed > 0 else 0
                logger.info(f"   Progress: {completed}/{total} ({rate:.0f}/s)")
    
    tasks = [validate_one(e) for e in entries]
    await asyncio.gather(*tasks, return_exceptions=True)


# ============================================================
# è§£æå™¨
# ============================================================

def is_valid_ip(ip: str) -> bool:
    """éªŒè¯ IPv4"""
    try:
        parts = ip.split('.')
        if len(parts) != 4:
            return False
        for part in parts:
            num = int(part)
            if num < 0 or num > 255:
                return False
        return not (ip.startswith('0.') or ip == '255.255.255.255')
    except ValueError:
        return False


def is_valid_port(port) -> bool:
    """éªŒè¯ç«¯å£"""
    try:
        p = int(port)
        return 1 <= p <= 65535
    except (ValueError, TypeError):
        return False


def parse_socks5_rich_line(line: str, source_name: str) -> Optional[IPEntry]:
    """è§£æå¯Œä¿¡æ¯ SOCKS5 è¡Œ"""
    line = line.strip()
    if not line or line.startswith('#'):
        return None
    
    match = SOCKS5_RICH_PATTERN.search(line)
    if match:
        ip, port_str, net_type, location_str, isp = match.groups()
        
        if not is_valid_ip(ip) or not is_valid_port(port_str):
            return None
        
        location_parts = location_str.strip().split()
        country = location_parts[0] if len(location_parts) > 0 else ""
        region = location_parts[1] if len(location_parts) > 1 else ""
        city = location_parts[2] if len(location_parts) > 2 else ""
        
        if len(location_parts) == 2:
            city = region
            region = ""
        
        return IPEntry(
            ip=ip,
            port=int(port_str),
            source=source_name,
            category="socks5",
            net_type=net_type.strip(),
            country=country,
            region=region,
            city=city,
            isp=isp.strip()
        )
    
    # å›é€€ç®€å•æ ¼å¼
    simple_match = PROXY_URL_PATTERN.search(line)
    if simple_match:
        ip, port_str = simple_match.groups()
        if is_valid_ip(ip) and is_valid_port(port_str):
            return IPEntry(ip=ip, port=int(port_str), source=source_name, category="socks5")
    
    return None


def parse_simple_line(line: str, source_name: str, category: str, region_hint: str = "") -> List[IPEntry]:
    """è§£æç®€å•æ ¼å¼è¡Œ"""
    results = []
    line = line.strip()
    
    if not line or line.startswith('#'):
        return results
    
    # ä»£ç† URL
    proxy_match = PROXY_URL_PATTERN.search(line)
    if proxy_match:
        ip, port_str = proxy_match.groups()
        if is_valid_ip(ip) and is_valid_port(port_str):
            entry = IPEntry(ip=ip, port=int(port_str), source=source_name, category=category)
            if region_hint:
                entry.country = region_hint
            results.append(entry)
        return results
    
    # IP:PORT æˆ– IP#PORT
    for match in LOOSE_IP_PORT_PATTERN.finditer(line):
        ip, port_str = match.groups()
        if is_valid_ip(ip) and is_valid_port(port_str):
            entry = IPEntry(ip=ip, port=int(port_str), source=source_name, category=category)
            if region_hint:
                entry.country = region_hint
            results.append(entry)
    
    # çº¯ IP
    if not results:
        for match in PURE_IP_PATTERN.finditer(line):
            ip = match.group(1)
            if is_valid_ip(ip):
                entry = IPEntry(ip=ip, source=source_name, category=category)
                if region_hint:
                    entry.country = region_hint
                results.append(entry)
    
    return results


def parse_text_content(content: str, source_name: str, category: str, region_hint: str = "") -> List[IPEntry]:
    """è§£æçº¯æ–‡æœ¬å†…å®¹"""
    entries = []
    for line in content.split('\n'):
        entries.extend(parse_simple_line(line, source_name, category, region_hint))
    return entries


def parse_socks5_rich_content(content: str, source_name: str) -> List[IPEntry]:
    """è§£æå¯Œä¿¡æ¯ SOCKS5 å†…å®¹"""
    entries = []
    for line in content.split('\n'):
        entry = parse_socks5_rich_line(line, source_name)
        if entry:
            entries.append(entry)
    return entries


def parse_html_content(content: str, source_name: str, category: str) -> List[IPEntry]:
    """è§£æ HTML å†…å®¹"""
    entries = []
    
    try:
        soup = BeautifulSoup(content, 'lxml')
        
        for table in soup.find_all('table'):
            for row in table.find_all('tr'):
                for cell in row.find_all(['td', 'th']):
                    text = cell.get_text(strip=True)
                    entries.extend(parse_simple_line(text, source_name, category))
        
        for tag in soup.find_all(['span', 'div', 'p', 'li', 'code', 'pre']):
            text = tag.get_text(strip=True)
            entries.extend(parse_simple_line(text, source_name, category))
        
        plain_text = soup.get_text(separator='\n')
        entries.extend(parse_text_content(plain_text, source_name, category))
        
    except Exception as e:
        logger.error(f"HTML parsing error: {e}")
        entries.extend(parse_text_content(content, source_name, category))
    
    return entries


# ============================================================
# æ•°æ®æºå¤„ç†
# ============================================================

def process_remote_source(source: Dict) -> List[IPEntry]:
    """å¤„ç†è¿œç¨‹æ•°æ®æº"""
    logger.info(f"ğŸ“¥ Remote: {source['name']}")
    
    content = fetch_url(source['url'])
    if not content:
        logger.warning(f"   âš ï¸ Empty content")
        return []
    
    source_type = source['type']
    source_name = source['name']
    category = source.get('category', 'unknown')
    
    if source_type == 'html':
        entries = parse_html_content(content, source_name, category)
    elif source_type == 'socks5_rich':
        entries = parse_socks5_rich_content(content, source_name)
    else:
        entries = parse_text_content(content, source_name, category)
    
    logger.info(f"   âœ… Found {len(entries)} entries")
    return entries


def process_local_source(source: Dict) -> List[IPEntry]:
    """å¤„ç†æœ¬åœ°æ•°æ®æº"""
    filepath = SCRIPT_DIR / source['file']
    logger.info(f"ğŸ“‚ Local: {source['name']} ({source['file']})")
    
    content = read_local_file(filepath)
    if not content:
        logger.warning(f"   âš ï¸ Empty or not found")
        return []
    
    source_name = source['name']
    category = source.get('category', 'local')
    region_hint = source.get('region_hint', '')
    
    entries = parse_text_content(content, source_name, category, region_hint)
    
    logger.info(f"   âœ… Found {len(entries)} entries")
    return entries


def deduplicate_entries(entries: List[IPEntry]) -> List[IPEntry]:
    """å»é‡ï¼ˆä¿ç•™ä¿¡æ¯æœ€ä¸°å¯Œçš„æ¡ç›®ï¼‰"""
    seen: Dict[str, IPEntry] = {}
    
    for entry in entries:
        key = entry.address
        
        if key not in seen:
            seen[key] = entry
        else:
            existing = seen[key]
            # ä¿ç•™ä¿¡æ¯æ›´ä¸°å¯Œçš„
            if entry.country and not existing.country:
                seen[key] = entry
            elif entry.net_type and not existing.net_type:
                existing.net_type = entry.net_type
                existing.country = entry.country or existing.country
                existing.region = entry.region or existing.region
                existing.city = entry.city or existing.city
                existing.isp = entry.isp or existing.isp
    
    return list(seen.values())


def sort_entries(entries: List[IPEntry]) -> List[IPEntry]:
    """æ’åº"""
    def sort_key(entry: IPEntry):
        try:
            octets = [int(x) for x in entry.ip.split('.')]
            return (0, octets, entry.port or 0)
        except ValueError:
            return (1, [0, 0, 0, 0], 0)
    
    return sorted(entries, key=sort_key)


# ============================================================
# å¯¼å‡ºå™¨
# ============================================================

class Exporter:
    """å¤šæ ¼å¼å¯¼å‡ºå™¨"""
    
    def __init__(self, output_dir: str = "output"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
    
    def export_all(self, entries: List[IPEntry], stats: Dict[str, Any]):
        """å¯¼å‡ºæ‰€æœ‰æ ¼å¼"""
        data = [e.to_dict() for e in entries]
        
        self._export_txt(entries, stats)
        self._export_json(data, stats)
        self._export_csv(data)
        self._export_valid_only(entries)
        self._export_summary(entries, stats)
        self._export_root_txt(entries)
        
        logger.info(f"ğŸ“ Exported to {self.output_dir}/")
    
    def _export_txt(self, entries: List[IPEntry], stats: Dict):
        """å¯¼å‡ºè¯¦ç»† TXT"""
        filepath = os.path.join(self.output_dir, "all.txt")
        
        valid_count = sum(1 for e in entries if e.is_valid is True)
        untested = sum(1 for e in entries if e.is_valid is None)
        
        lines = [
            "# " + "=" * 70,
            "# Aggregated IP/Proxy Addresses",
            f"# Generated: {self.timestamp}",
            f"# Total: {len(entries)} | Valid: {valid_count} | Untested: {untested}",
            "# " + "=" * 70,
            "# Format: ADDRESS | STATUS | LATENCY | TYPE | LOCATION | ISP",
            "# " + "=" * 70,
            ""
        ]
        
        for e in entries:
            if e.is_valid is True:
                status = "âœ“"
            elif e.is_valid is False:
                status = "âœ—"
            else:
                status = "?"
            
            latency = f"{e.latency_ms:.0f}ms" if e.latency_ms else "-"
            net_type = e.net_type or "-"
            location = e.location or "-"
            isp = e.isp[:25] if e.isp else "-"
            
            lines.append(f"{e.address:<22} | {status} | {latency:<7} | {net_type:<4} | {location:<15} | {isp}")
        
        lines.append("")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
    
    def _export_json(self, data: List[Dict], stats: Dict):
        """å¯¼å‡º JSON"""
        filepath = os.path.join(self.output_dir, "all.json")
        
        by_country = {}
        by_net_type = {"datacenter": 0, "residential": 0, "unknown": 0}
        by_source = {}
        by_category = {}
        
        for item in data:
            country = item.get('country') or 'Unknown'
            by_country[country] = by_country.get(country, 0) + 1
            
            net_type = item.get('net_type_en', 'unknown')
            by_net_type[net_type] = by_net_type.get(net_type, 0) + 1
            
            source = item.get('source', 'unknown')
            by_source[source] = by_source.get(source, 0) + 1
            
            cat = item.get('category', 'unknown')
            by_category[cat] = by_category.get(cat, 0) + 1
        
        latencies = [d['latency_ms'] for d in data if d.get('latency_ms')]
        latency_stats = {}
        if latencies:
            latencies.sort()
            latency_stats = {
                "min": min(latencies),
                "max": max(latencies),
                "avg": round(sum(latencies) / len(latencies), 2),
                "median": latencies[len(latencies) // 2]
            }
        
        output = {
            "metadata": {
                "generated_at": self.timestamp,
                "total_count": len(data),
                "valid_count": sum(1 for d in data if d.get('is_valid') is True),
                "invalid_count": sum(1 for d in data if d.get('is_valid') is False),
                "untested_count": sum(1 for d in data if d.get('is_valid') is None),
                "validated": not SKIP_VALIDATION
            },
            "statistics": {
                "by_country": dict(sorted(by_country.items(), key=lambda x: x[1], reverse=True)),
                "by_net_type": by_net_type,
                "by_source": by_source,
                "by_category": by_category,
                "latency": latency_stats
            },
            "data": data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
    
    def _export_csv(self, data: List[Dict]):
        """å¯¼å‡º CSV"""
        filepath = os.path.join(self.output_dir, "all.csv")
        
        if not data:
            return
        
        fieldnames = [
            'address', 'ip', 'port', 'is_valid', 'latency_ms',
            'net_type', 'net_type_en', 'country', 'region', 'city', 'isp',
            'location', 'source', 'category', 'validation_error'
        ]
        
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(data)
    
    def _export_valid_only(self, entries: List[IPEntry]):
        """
        å¯¼å‡ºæœ‰æ•ˆ IP åˆ—è¡¨
        - éªŒè¯é€šè¿‡çš„ (is_valid=True)
        - æœªéªŒè¯çš„ä¹ŸåŒ…å« (is_valid=None)ï¼Œé™¤é SKIP_VALIDATION=false
        """
        filepath = os.path.join(self.output_dir, "valid_only.txt")
        
        if SKIP_VALIDATION:
            # è·³è¿‡éªŒè¯æ—¶ï¼Œè¾“å‡ºæ‰€æœ‰
            valid = entries
        else:
            # ä»…è¾“å‡ºéªŒè¯é€šè¿‡çš„
            valid = [e for e in entries if e.is_valid is True]
        
        # æŒ‰å»¶è¿Ÿæ’åºï¼ˆæœ‰å»¶è¿Ÿçš„åœ¨å‰ï¼‰
        valid_sorted = sorted(valid, key=lambda x: (x.latency_ms is None, x.latency_ms or 9999))
        
        lines = [
            f"# ========================================",
            f"# Valid IP Addresses",
            f"# Generated: {self.timestamp}",
            f"# Count: {len(valid_sorted)}",
            f"# ========================================",
            f"# Sorted by latency (fastest first)",
            f"# ========================================",
            ""
        ]
        
        for e in valid_sorted:
            # æ ¼å¼: IP:PORT  # latency | location | isp
            comment_parts = []
            if e.latency_ms:
                comment_parts.append(f"{e.latency_ms:.0f}ms")
            if e.net_type:
                comment_parts.append(e.net_type)
            if e.location:
                comment_parts.append(e.location)
            if e.isp:
                comment_parts.append(e.isp[:20])
            
            if comment_parts:
                lines.append(f"{e.address}  # {' | '.join(comment_parts)}")
            else:
                lines.append(e.address)
        
        lines.append("")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        logger.info(f"   ğŸ“„ valid_only.txt: {len(valid_sorted)} entries")
    
    def _export_summary(self, entries: List[IPEntry], stats: Dict):
        """å¯¼å‡º Markdown æ‘˜è¦"""
        filepath = os.path.join(self.output_dir, "summary.md")
        
        total = len(entries)
        valid = sum(1 for e in entries if e.is_valid is True)
        invalid = sum(1 for e in entries if e.is_valid is False)
        untested = sum(1 for e in entries if e.is_valid is None)
        
        # æŒ‰æ¥æºç»Ÿè®¡
        source_counts = {}
        for e in entries:
            source_counts[e.source] = source_counts.get(e.source, 0) + 1
        
        # å›½å®¶ç»Ÿè®¡
        country_counts = {}
        for e in entries:
            c = e.country or 'Unknown'
            country_counts[c] = country_counts.get(c, 0) + 1
        top_countries = sorted(country_counts.items(), key=lambda x: x[1], reverse=True)[:15]
        
        # ç½‘ç»œç±»å‹ç»Ÿè®¡
        net_counts = {"æœºæˆ¿": 0, "å®¶å®½": 0, "æœªçŸ¥": 0}
        for e in entries:
            if e.net_type == "æœºæˆ¿":
                net_counts["æœºæˆ¿"] += 1
            elif e.net_type == "å®¶å®½":
                net_counts["å®¶å®½"] += 1
            else:
                net_counts["æœªçŸ¥"] += 1
        
        # æœ€å¿« IP
        valid_with_latency = [e for e in entries if e.is_valid and e.latency_ms]
        fastest = sorted(valid_with_latency, key=lambda x: x.latency_ms)[:20]
        
        md = f"""# ğŸ“Š IP Aggregation Report

> **Generated:** {self.timestamp}

## ğŸ“ˆ Overview

| Metric | Value |
|--------|-------|
| **Total Entries** | {total} |
| **âœ… Valid** | {valid} ({valid/total*100:.1f}%) |
| **âŒ Invalid** | {invalid} ({invalid/total*100:.1f}%) |
| **â“ Untested** | {untested} ({untested/total*100:.1f}%) |

## ğŸ“¡ Data Sources

| Source | Count | Type |
|--------|-------|------|
"""
        for name, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):
            src_type = "ğŸŒ Remote" if any(s['name'] == name for s in REMOTE_SOURCES) else "ğŸ“‚ Local"
            md += f"| {name} | {count} | {src_type} |\n"
        
        md += f"""
## ğŸ  Network Type Distribution

| Type | Count | Percentage |
|------|-------|------------|
| ğŸ¢ æœºæˆ¿ (Datacenter) | {net_counts['æœºæˆ¿']} | {net_counts['æœºæˆ¿']/total*100:.1f}% |
| ğŸ  å®¶å®½ (Residential) | {net_counts['å®¶å®½']} | {net_counts['å®¶å®½']/total*100:.1f}% |
| â“ æœªçŸ¥ (Unknown) | {net_counts['æœªçŸ¥']} | {net_counts['æœªçŸ¥']/total*100:.1f}% |

## ğŸŒ Geographic Distribution (Top 15)

| Country | Count | Percentage |
|---------|-------|------------|
"""
        for country, count in top_countries:
            pct = count / total * 100
            md += f"| {country} | {count} | {pct:.1f}% |\n"
        
        md += f"""
## âš¡ Top 20 Fastest IPs

| # | Address | Latency | Type | Location | ISP |
|---|---------|---------|------|----------|-----|
"""
        for i, e in enumerate(fastest, 1):
            net = e.net_type or "-"
            loc = e.location or "-"
            isp = (e.isp[:20] + "...") if e.isp and len(e.isp) > 20 else (e.isp or "-")
            md += f"| {i} | `{e.address}` | {e.latency_ms:.0f}ms | {net} | {loc} | {isp} |\n"
        
        md += """
---

## ğŸ“ Output Files

| File | Description |
|------|-------------|
| `all.txt` | All entries with details |
| `all.json` | Full data in JSON format |
| `all.csv` | Spreadsheet format |
| `valid_only.txt` | Only valid IPs (fastest first) |
| `summary.md` | This report |

---
*Auto-generated by IP Aggregation System v5.0*
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md)
    
    def _export_root_txt(self, entries: List[IPEntry]):
        """æ ¹ç›®å½•ç®€æ´æ ¼å¼"""
        filepath = "all.txt"
        
        lines = [
            f"# Aggregated IPs - {self.timestamp}",
            f"# Total: {len(entries)}",
            ""
        ]
        lines.extend([e.address for e in entries])
        lines.append("")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))


# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    logger.info("=" * 60)
    logger.info("ğŸš€ IP Aggregation System v5.0")
    logger.info("=" * 60)
    logger.info(f"âš™ï¸  Validation: {'SKIP' if SKIP_VALIDATION else 'ENABLED'}")
    if not SKIP_VALIDATION:
        logger.info(f"âš™ï¸  Timeout: {VALIDATION_TIMEOUT}s | Concurrency: {VALIDATION_CONCURRENCY}")
    logger.info(f"âš™ï¸  Script dir: {SCRIPT_DIR}")
    logger.info("=" * 60)
    
    # ===== é˜¶æ®µ 1: æ•°æ®é‡‡é›† =====
    logger.info("\nğŸ“¡ PHASE 1: Data Collection")
    logger.info("-" * 40)
    
    all_entries: List[IPEntry] = []
    source_stats: Dict[str, int] = {}
    
    # è¿œç¨‹æº
    logger.info("\nğŸŒ Remote sources:")
    for source in REMOTE_SOURCES:
        try:
            entries = process_remote_source(source)
            all_entries.extend(entries)
            source_stats[source['name']] = len(entries)
        except Exception as e:
            logger.error(f"   âŒ Error: {source['name']}: {e}")
            source_stats[source['name']] = 0
    
    # æœ¬åœ°æº
    logger.info("\nğŸ“‚ Local sources:")
    for source in LOCAL_SOURCES:
        try:
            entries = process_local_source(source)
            all_entries.extend(entries)
            source_stats[source['name']] = len(entries)
        except Exception as e:
            logger.error(f"   âŒ Error: {source['name']}: {e}")
            source_stats[source['name']] = 0
    
    logger.info(f"\nğŸ“Š Raw total: {len(all_entries)} entries")
    
    # ===== é˜¶æ®µ 2: å»é‡æ’åº =====
    logger.info("\nğŸ”„ PHASE 2: Deduplication & Sorting")
    logger.info("-" * 40)
    
    unique_entries = deduplicate_entries(all_entries)
    unique_entries = sort_entries(unique_entries)
    
    logger.info(f"ğŸ“Š Unique entries: {len(unique_entries)}")
    
    # ===== é˜¶æ®µ 3: éªŒè¯ =====
    if not SKIP_VALIDATION and unique_entries:
        logger.info("\nğŸ” PHASE 3: Validation")
        logger.info("-" * 40)
        logger.info(f"   Testing {len(unique_entries)} addresses...")
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(
                validate_entries_async(
                    unique_entries,
                    timeout=VALIDATION_TIMEOUT,
                    concurrency=VALIDATION_CONCURRENCY
                )
            )
        finally:
            loop.close()
        
        valid_count = sum(1 for e in unique_entries if e.is_valid)
        invalid_count = sum(1 for e in unique_entries if e.is_valid is False)
        logger.info(f"\nğŸ“Š Results: âœ… {valid_count} valid | âŒ {invalid_count} invalid")
    else:
        logger.info("\nâ­ï¸  PHASE 3: Validation SKIPPED")
    
    # ===== é˜¶æ®µ 4: å¯¼å‡º =====
    logger.info("\nğŸ’¾ PHASE 4: Export")
    logger.info("-" * 40)
    
    stats = {'sources': source_stats}
    exporter = Exporter(OUTPUT_DIR)
    exporter.export_all(unique_entries, stats)
    
    # ===== å®Œæˆ =====
    elapsed = time.time() - start_time
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ¨ COMPLETED")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Total: {len(unique_entries)} entries")
    if not SKIP_VALIDATION:
        valid = sum(1 for e in unique_entries if e.is_valid)
        logger.info(f"âœ… Valid: {valid}")
    logger.info(f"â±ï¸  Time: {elapsed:.1f}s")
    logger.info("=" * 60)
    
    # è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
    logger.info("\nğŸ“ Output files:")
    for f in ["all.txt", "all.json", "all.csv", "valid_only.txt", "summary.md"]:
        filepath = os.path.join(OUTPUT_DIR, f)
        if os.path.exists(filepath):
            size = os.path.getsize(filepath)
            logger.info(f"   âœ“ {OUTPUT_DIR}/{f} ({size:,} bytes)")
    if os.path.exists("all.txt"):
        logger.info(f"   âœ“ all.txt (root)")


if __name__ == "__main__":
    main()
