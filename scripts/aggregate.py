import os
import re
import csv
import json
import time
import asyncio
import socket
from datetime import datetime, timezone
from typing import Set, Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor
import logging

import requests
from bs4 import BeautifulSoup

# ============================================================
# é…ç½®
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-7s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# ç¯å¢ƒå˜é‡é…ç½®
SKIP_VALIDATION = os.environ.get('SKIP_VALIDATION', 'false').lower() == 'true'
VALIDATION_TIMEOUT = float(os.environ.get('VALIDATION_TIMEOUT', '3'))
VALIDATION_CONCURRENCY = int(os.environ.get('VALIDATION_CONCURRENCY', '100'))
OUTPUT_DIR = os.environ.get('OUTPUT_DIR', 'output')

# ============================================================
# æ•°æ®æºé…ç½®
# ============================================================

SOURCES = [
    {
        "name": "ipTop10.html",
        "url": "https://raw.githubusercontent.com/chnbsdan/cf-speed-dns/refs/heads/main/ipTop10.html",
        "type": "html",
        "category": "cloudflare"
    },
    {
        "name": "edgetunnel-output",
        "url": "https://raw.githubusercontent.com/chnbsdan/edgetunnel3/refs/heads/main/output.txt",
        "type": "text",
        "category": "cloudflare"
    },
    {
        "name": "bestproxy",
        "url": "https://ipdb.api.030101.xyz/?type=bestproxy&country=true",
        "type": "text",
        "category": "proxy"
    },
    {
        "name": "bestcf",
        "url": "https://ipdb.api.030101.xyz/?type=bestcf",
        "type": "text",
        "category": "proxy"
    },
    {
        "name": "socks5-proxy",
        "url": "https://raw.githubusercontent.com/chnbsdan/free-proxy-list/refs/heads/main/proxy.txt",
        "type": "socks5_rich",
        "category": "socks5"
    }
]

# ============================================================
# æ•°æ®ç»“æ„
# ============================================================

@dataclass
class IPEntry:
    """IP æ¡ç›®æ•°æ®ç»“æ„"""
    ip: str
    port: Optional[int] = None
    
    # æ¥æºä¿¡æ¯
    source: str = ""
    category: str = ""
    
    # åœ°ç†ä¿¡æ¯ï¼ˆä»æºæ•°æ®æˆ– API è·å–ï¼‰
    country: str = ""
    region: str = ""
    city: str = ""
    isp: str = ""
    
    # ç½‘ç»œç±»å‹
    net_type: str = ""  # æœºæˆ¿ / å®¶å®½ / unknown
    
    # éªŒè¯ç»“æœ
    is_valid: Optional[bool] = None
    latency_ms: Optional[float] = None
    validation_error: str = ""
    
    @property
    def address(self) -> str:
        """å®Œæ•´åœ°å€ IP:PORT"""
        if self.port:
            return f"{self.ip}:{self.port}"
        return self.ip
    
    @property
    def location(self) -> str:
        """ä½ç½®ç®€è¿°"""
        parts = []
        if self.country:
            parts.append(self.country)
        if self.city:
            parts.append(self.city)
        elif self.region:
            parts.append(self.region)
        return " ".join(parts) if parts else "Unknown"
    
    @property
    def net_type_en(self) -> str:
        """ç½‘ç»œç±»å‹è‹±æ–‡"""
        mapping = {
            "æœºæˆ¿": "datacenter",
            "å®¶å®½": "residential",
            "": "unknown"
        }
        return mapping.get(self.net_type, "unknown")
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "address": self.address,
            "ip": self.ip,
            "port": self.port,
            "source": self.source,
            "category": self.category,
            "country": self.country,
            "region": self.region,
            "city": self.city,
            "isp": self.isp,
            "net_type": self.net_type,
            "net_type_en": self.net_type_en,
            "location": self.location,
            "is_valid": self.is_valid,
            "latency_ms": self.latency_ms,
            "validation_error": self.validation_error
        }


# ============================================================
# æ­£åˆ™è¡¨è¾¾å¼
# ============================================================

IPV4_PATTERN = r'(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)'

# æ ‡å‡†ä»£ç† URL æ ¼å¼: protocol://IP:PORT
PROXY_URL_PATTERN = re.compile(
    r'(?:socks[45]?|https?|ss|ssr|vmess|trojan)://'
    r'(?:[^:@\s]+:[^:@\s]+@)?'  # å¯é€‰è®¤è¯
    rf'({IPV4_PATTERN}):(\d{{1,5}})',
    re.IGNORECASE
)

# å¯Œä¿¡æ¯ SOCKS5 æ ¼å¼: socks5://IP:PORT [[ç±»å‹] å›½å®¶ çœ åŸå¸‚ [ISP]]
SOCKS5_RICH_PATTERN = re.compile(
    rf'socks[45]?://({IPV4_PATTERN}):(\d{{1,5}})'  # IP:PORT
    r'\s*'
    r'\[\[([^\]]*)\]\s*'  # [[ç±»å‹]
    r'([^\[]*?)'  # å›½å®¶ çœ åŸå¸‚
    r'\[([^\]]*)\]\]',  # [ISP]]
    re.IGNORECASE
)

# å¤‡ç”¨ï¼šæ›´å®½æ¾çš„å¯Œä¿¡æ¯åŒ¹é…
SOCKS5_RICH_PATTERN_ALT = re.compile(
    rf'socks[45]?://({IPV4_PATTERN}):(\d{{1,5}})'
    r'\s*\[\['
    r'(æœºæˆ¿|å®¶å®½)'
    r'\]\s*'
    r'([^\[]+?)'
    r'\s*\[([^\]]+)\]\]',
    re.IGNORECASE
)

# IP:PORT æˆ– IP#PORT
LOOSE_IP_PORT_PATTERN = re.compile(rf'\b({IPV4_PATTERN})[:#](\d{{1,5}})\b')

# çº¯ IP
PURE_IP_PATTERN = re.compile(rf'\b({IPV4_PATTERN})\b')


# ============================================================
# ç½‘ç»œå·¥å…·
# ============================================================

def fetch_content(url: str, timeout: int = 30, retries: int = 3) -> str:
    """è·å– URL å†…å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    }
    
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            logger.warning(f"Attempt {attempt + 1}/{retries} failed: {e}")
            if attempt < retries - 1:
                time.sleep(1)
    return ""


def tcp_ping(ip: str, port: int, timeout: float = 3.0) -> Tuple[bool, Optional[float], str]:
    """TCP è¿æ¥æµ‹è¯•"""
    try:
        start = time.time()
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((ip, port))
        elapsed = (time.time() - start) * 1000
        sock.close()
        
        if result == 0:
            return True, round(elapsed, 2), ""
        return False, None, f"Connection failed (code: {result})"
    except socket.timeout:
        return False, None, "Timeout"
    except Exception as e:
        return False, None, str(e)


async def async_tcp_ping(ip: str, port: int, timeout: float = 3.0) -> Tuple[bool, Optional[float], str]:
    """å¼‚æ­¥ TCP æµ‹è¯•"""
    try:
        start = time.time()
        _, writer = await asyncio.wait_for(
            asyncio.open_connection(ip, port),
            timeout=timeout
        )
        elapsed = (time.time() - start) * 1000
        writer.close()
        await writer.wait_closed()
        return True, round(elapsed, 2), ""
    except asyncio.TimeoutError:
        return False, None, "Timeout"
    except ConnectionRefusedError:
        return False, None, "Connection refused"
    except Exception as e:
        return False, None, str(e)


async def validate_entries_async(
    entries: List[IPEntry],
    timeout: float = 3.0,
    concurrency: int = 100
) -> None:
    """æ‰¹é‡å¼‚æ­¥éªŒè¯ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰"""
    semaphore = asyncio.Semaphore(concurrency)
    completed = 0
    total = len(entries)
    
    async def validate_one(entry: IPEntry):
        nonlocal completed
        async with semaphore:
            if entry.port:
                success, latency, error = await async_tcp_ping(entry.ip, entry.port, timeout)
            else:
                # æ— ç«¯å£æ—¶æµ‹è¯•å¸¸ç”¨ç«¯å£
                success, latency, error = False, None, "No port"
                for test_port in [443, 80, 8080, 1080]:
                    success, latency, error = await async_tcp_ping(entry.ip, test_port, timeout / 4)
                    if success:
                        break
            
            entry.is_valid = success
            entry.latency_ms = latency
            entry.validation_error = error
            
            completed += 1
            if completed % 100 == 0:
                logger.info(f"   Validated: {completed}/{total}")
    
    tasks = [validate_one(e) for e in entries]
    await asyncio.gather(*tasks, return_exceptions=True)


# ============================================================
# è§£æå™¨
# ============================================================

def is_valid_ip(ip: str) -> bool:
    """éªŒè¯ IPv4"""
    try:
        parts = ip.split('.')
        if len(parts) != 4:
            return False
        for part in parts:
            num = int(part)
            if num < 0 or num > 255:
                return False
        return not (ip.startswith('0.') or ip == '255.255.255.255')
    except ValueError:
        return False


def is_valid_port(port) -> bool:
    """éªŒè¯ç«¯å£"""
    try:
        p = int(port)
        return 1 <= p <= 65535
    except (ValueError, TypeError):
        return False


def parse_socks5_rich_line(line: str, source_name: str) -> Optional[IPEntry]:
    """
    è§£æå¯Œä¿¡æ¯ SOCKS5 è¡Œ
    æ ¼å¼: socks5://IP:PORT [[ç±»å‹] å›½å®¶ çœ åŸå¸‚ [ISP]]
    """
    line = line.strip()
    if not line or line.startswith('#'):
        return None
    
    # å°è¯•ä¸»æ­£åˆ™
    match = SOCKS5_RICH_PATTERN.search(line)
    if not match:
        match = SOCKS5_RICH_PATTERN_ALT.search(line)
    
    if match:
        ip, port_str, net_type, location_str, isp = match.groups()
        
        if not is_valid_ip(ip) or not is_valid_port(port_str):
            return None
        
        # è§£æä½ç½®å­—ç¬¦ä¸²: "éŸ©å›½ é¦–å°”ç‰¹åˆ«å¸‚ é¦–å°”ç‰¹åˆ«å¸‚" æˆ– "æ„å¤§åˆ© æ™®åˆ©äºš"
        location_parts = location_str.strip().split()
        country = location_parts[0] if len(location_parts) > 0 else ""
        region = location_parts[1] if len(location_parts) > 1 else ""
        city = location_parts[2] if len(location_parts) > 2 else ""
        
        # å¦‚æœåªæœ‰ä¸¤éƒ¨åˆ†ï¼Œç¬¬äºŒéƒ¨åˆ†å¯èƒ½æ˜¯åŸå¸‚
        if len(location_parts) == 2:
            city = region
            region = ""
        
        return IPEntry(
            ip=ip,
            port=int(port_str),
            source=source_name,
            category="socks5",
            net_type=net_type.strip(),
            country=country,
            region=region,
            city=city,
            isp=isp.strip()
        )
    
    # å›é€€åˆ°ç®€å•æ ¼å¼
    simple_match = PROXY_URL_PATTERN.search(line)
    if simple_match:
        ip, port_str = simple_match.groups()
        if is_valid_ip(ip) and is_valid_port(port_str):
            return IPEntry(
                ip=ip,
                port=int(port_str),
                source=source_name,
                category="socks5"
            )
    
    return None


def parse_simple_line(line: str, source_name: str, category: str) -> List[IPEntry]:
    """è§£æç®€å•æ ¼å¼è¡Œï¼ˆIP æˆ– IP:PORTï¼‰"""
    results = []
    line = line.strip()
    
    if not line or line.startswith('#'):
        return results
    
    # å°è¯•ä»£ç† URL
    proxy_match = PROXY_URL_PATTERN.search(line)
    if proxy_match:
        ip, port_str = proxy_match.groups()
        if is_valid_ip(ip) and is_valid_port(port_str):
            results.append(IPEntry(
                ip=ip,
                port=int(port_str),
                source=source_name,
                category=category
            ))
        return results
    
    # å°è¯• IP:PORT
    for match in LOOSE_IP_PORT_PATTERN.finditer(line):
        ip, port_str = match.groups()
        if is_valid_ip(ip) and is_valid_port(port_str):
            results.append(IPEntry(
                ip=ip,
                port=int(port_str),
                source=source_name,
                category=category
            ))
    
    # å°è¯•çº¯ IP
    if not results:
        for match in PURE_IP_PATTERN.finditer(line):
            ip = match.group(1)
            if is_valid_ip(ip):
                results.append(IPEntry(
                    ip=ip,
                    source=source_name,
                    category=category
                ))
    
    return results


def parse_text_content(content: str, source_name: str, category: str) -> List[IPEntry]:
    """è§£æçº¯æ–‡æœ¬å†…å®¹"""
    entries = []
    for line in content.split('\n'):
        entries.extend(parse_simple_line(line, source_name, category))
    return entries


def parse_socks5_rich_content(content: str, source_name: str) -> List[IPEntry]:
    """è§£æå¯Œä¿¡æ¯ SOCKS5 å†…å®¹"""
    entries = []
    for line in content.split('\n'):
        entry = parse_socks5_rich_line(line, source_name)
        if entry:
            entries.append(entry)
    return entries


def parse_html_content(content: str, source_name: str, category: str) -> List[IPEntry]:
    """è§£æ HTML å†…å®¹"""
    entries = []
    
    try:
        soup = BeautifulSoup(content, 'lxml')
        
        # ä»è¡¨æ ¼æå–
        for table in soup.find_all('table'):
            for row in table.find_all('tr'):
                for cell in row.find_all(['td', 'th']):
                    text = cell.get_text(strip=True)
                    entries.extend(parse_simple_line(text, source_name, category))
        
        # ä»å…¶ä»–æ ‡ç­¾æå–
        for tag in soup.find_all(['span', 'div', 'p', 'li', 'code', 'pre']):
            text = tag.get_text(strip=True)
            entries.extend(parse_simple_line(text, source_name, category))
        
        # å…œåº•ï¼šçº¯æ–‡æœ¬
        plain_text = soup.get_text(separator='\n')
        entries.extend(parse_text_content(plain_text, source_name, category))
        
    except Exception as e:
        logger.error(f"HTML parsing error: {e}")
        entries.extend(parse_text_content(content, source_name, category))
    
    return entries


# ============================================================
# æ•°æ®æºå¤„ç†
# ============================================================

def process_source(source: Dict) -> List[IPEntry]:
    """å¤„ç†å•ä¸ªæ•°æ®æº"""
    logger.info(f"ğŸ“¥ Fetching: {source['name']}")
    
    content = fetch_content(source['url'])
    if not content:
        logger.warning(f"âš ï¸  Empty: {source['name']}")
        return []
    
    source_type = source['type']
    source_name = source['name']
    category = source.get('category', 'unknown')
    
    if source_type == 'html':
        entries = parse_html_content(content, source_name, category)
    elif source_type == 'socks5_rich':
        entries = parse_socks5_rich_content(content, source_name)
    else:
        entries = parse_text_content(content, source_name, category)
    
    logger.info(f"âœ… Found {len(entries)} entries from {source_name}")
    return entries


def deduplicate_entries(entries: List[IPEntry]) -> List[IPEntry]:
    """å»é‡ï¼ˆä¿ç•™ä¿¡æ¯æœ€ä¸°å¯Œçš„æ¡ç›®ï¼‰"""
    seen: Dict[str, IPEntry] = {}
    
    for entry in entries:
        key = entry.address
        
        if key not in seen:
            seen[key] = entry
        else:
            # ä¿ç•™ä¿¡æ¯æ›´ä¸°å¯Œçš„
            existing = seen[key]
            # å¦‚æœæ–°æ¡ç›®æœ‰åœ°ç†ä¿¡æ¯è€Œæ—§çš„æ²¡æœ‰ï¼Œæ›¿æ¢
            if entry.country and not existing.country:
                seen[key] = entry
            # å¦‚æœæ–°æ¡ç›®æœ‰ç½‘ç»œç±»å‹è€Œæ—§çš„æ²¡æœ‰ï¼Œåˆå¹¶
            elif entry.net_type and not existing.net_type:
                existing.net_type = entry.net_type
                existing.country = entry.country or existing.country
                existing.region = entry.region or existing.region
                existing.city = entry.city or existing.city
                existing.isp = entry.isp or existing.isp
    
    return list(seen.values())


def sort_entries(entries: List[IPEntry]) -> List[IPEntry]:
    """æ’åº"""
    def sort_key(entry: IPEntry):
        try:
            octets = [int(x) for x in entry.ip.split('.')]
            return (0, octets, entry.port or 0)
        except ValueError:
            return (1, [0, 0, 0, 0], 0)
    
    return sorted(entries, key=sort_key)


# ============================================================
# å¯¼å‡ºå™¨
# ============================================================

class Exporter:
    """å¤šæ ¼å¼å¯¼å‡ºå™¨"""
    
    def __init__(self, output_dir: str = "output"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
    
    def export_all(self, entries: List[IPEntry], stats: Dict[str, Any]):
        """å¯¼å‡ºæ‰€æœ‰æ ¼å¼"""
        data = [e.to_dict() for e in entries]
        
        self._export_txt(entries, stats)
        self._export_json(data, stats)
        self._export_csv(data)
        self._export_valid_only(entries)
        self._export_summary(entries, stats)
        self._export_root_txt(entries)
        
        logger.info(f"ğŸ“ Exported to {self.output_dir}/")
    
    def _export_txt(self, entries: List[IPEntry], stats: Dict):
        """å¯¼å‡ºè¯¦ç»† TXT"""
        filepath = os.path.join(self.output_dir, "all.txt")
        
        valid_count = sum(1 for e in entries if e.is_valid is True or e.is_valid is None)
        
        lines = [
            "# " + "=" * 70,
            "# Aggregated IP/Proxy Addresses",
            f"# Generated: {self.timestamp}",
            f"# Total: {len(entries)} | Valid: {valid_count}",
            "# " + "=" * 70,
            "# Format: ADDRESS | TYPE | LATENCY | LOCATION | ISP",
            "# " + "=" * 70,
            ""
        ]
        
        for e in entries:
            status = "âœ“" if e.is_valid else ("âœ—" if e.is_valid is False else "?")
            latency = f"{e.latency_ms:.0f}ms" if e.latency_ms else "-"
            net_type = e.net_type or "-"
            location = e.location or "-"
            isp = e.isp[:30] if e.isp else "-"
            
            lines.append(f"{e.address:<22} | {status} {net_type:<4} | {latency:<8} | {location:<20} | {isp}")
        
        lines.append("")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
    
    def _export_json(self, data: List[Dict], stats: Dict):
        """å¯¼å‡º JSON"""
        filepath = os.path.join(self.output_dir, "all.json")
        
        # ç»Ÿè®¡
        by_country = {}
        by_net_type = {"datacenter": 0, "residential": 0, "unknown": 0}
        by_source = {}
        
        for item in data:
            country = item.get('country') or 'Unknown'
            by_country[country] = by_country.get(country, 0) + 1
            
            net_type = item.get('net_type_en', 'unknown')
            by_net_type[net_type] = by_net_type.get(net_type, 0) + 1
            
            source = item.get('source', 'unknown')
            by_source[source] = by_source.get(source, 0) + 1
        
        # å»¶è¿Ÿç»Ÿè®¡
        latencies = [d['latency_ms'] for d in data if d.get('latency_ms')]
        latency_stats = {}
        if latencies:
            latencies.sort()
            latency_stats = {
                "min": min(latencies),
                "max": max(latencies),
                "avg": round(sum(latencies) / len(latencies), 2),
                "median": latencies[len(latencies) // 2]
            }
        
        output = {
            "metadata": {
                "generated_at": self.timestamp,
                "total_count": len(data),
                "valid_count": sum(1 for d in data if d.get('is_valid') is True or d.get('is_valid') is None),
                "validated": not SKIP_VALIDATION
            },
            "statistics": {
                "by_country": dict(sorted(by_country.items(), key=lambda x: x[1], reverse=True)),
                "by_net_type": by_net_type,
                "by_source": by_source,
                "latency": latency_stats
            },
            "data": data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
    
    def _export_csv(self, data: List[Dict]):
        """å¯¼å‡º CSV"""
        filepath = os.path.join(self.output_dir, "all.csv")
        
        if not data:
            return
        
        fieldnames = [
            'address', 'ip', 'port', 'net_type', 'net_type_en',
            'country', 'region', 'city', 'isp', 'location',
            'is_valid', 'latency_ms', 'source', 'category'
        ]
        
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(data)
    
    def _export_valid_only(self, entries: List[IPEntry]):
        """å¯¼å‡ºä»…æœ‰æ•ˆ IP"""
        filepath = os.path.join(self.output_dir, "valid_only.txt")
        
        valid = [e for e in entries if e.is_valid is True or e.is_valid is None]
        
        lines = [
            f"# Valid IPs - {self.timestamp}",
            f"# Count: {len(valid)}",
            ""
        ]
        lines.extend([e.address for e in valid])
        lines.append("")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
    
    def _export_summary(self, entries: List[IPEntry], stats: Dict):
        """å¯¼å‡º Markdown æ‘˜è¦"""
        filepath = os.path.join(self.output_dir, "summary.md")
        
        total = len(entries)
        valid = sum(1 for e in entries if e.is_valid is True)
        invalid = sum(1 for e in entries if e.is_valid is False)
        untested = total - valid - invalid
        
        # å›½å®¶ç»Ÿè®¡
        country_counts = {}
        for e in entries:
            c = e.country or 'Unknown'
            country_counts[c] = country_counts.get(c, 0) + 1
        top_countries = sorted(country_counts.items(), key=lambda x: x[1], reverse=True)[:15]
        
        # ç½‘ç»œç±»å‹ç»Ÿè®¡
        net_type_counts = {"æœºæˆ¿": 0, "å®¶å®½": 0, "æœªçŸ¥": 0}
        for e in entries:
            if e.net_type == "æœºæˆ¿":
                net_type_counts["æœºæˆ¿"] += 1
            elif e.net_type == "å®¶å®½":
                net_type_counts["å®¶å®½"] += 1
            else:
                net_type_counts["æœªçŸ¥"] += 1
        
        # æœ€å¿« IP
        valid_with_latency = [e for e in entries if e.is_valid and e.latency_ms]
        fastest = sorted(valid_with_latency, key=lambda x: x.latency_ms)[:15]
        
        md = f"""# ğŸ“Š IP Aggregation Report

> **Generated:** {self.timestamp}

## ğŸ“ˆ Overview

| Metric | Value |
|--------|-------|
| **Total Entries** | {total} |
| **âœ… Valid** | {valid} ({valid/total*100:.1f}% ) |
| **âŒ Invalid** | {invalid} ({invalid/total*100:.1f}%) |
| **â“ Untested** | {untested} |

## ğŸ“¡ Sources

| Source | Count |
|--------|-------|
"""
        for name, count in stats.get('sources', {}).items():
            md += f"| {name} | {count} |\n"
        
        md += f"""
## ğŸ  Network Type Distribution

| Type | Count | Percentage |
|------|-------|------------|
| ğŸ¢ æœºæˆ¿ (Datacenter) | {net_type_counts['æœºæˆ¿']} | {net_type_counts['æœºæˆ¿']/total*100:.1f}% |
| ğŸ  å®¶å®½ (Residential) | {net_type_counts['å®¶å®½']} | {net_type_counts['å®¶å®½']/total*100:.1f}% |
| â“ æœªçŸ¥ (Unknown) | {net_type_counts['æœªçŸ¥']} | {net_type_counts['æœªçŸ¥']/total*100:.1f}% |

## ğŸŒ Geographic Distribution (Top 15)

| Country | Count | Percentage |
|---------|-------|------------|
"""
        for country, count in top_countries:
            pct = count / total * 100
            md += f"| {country} | {count} | {pct:.1f}% |\n"
        
        md += f"""
## âš¡ Top 15 Fastest IPs

| Address | Latency | Type | Location | ISP |
|---------|---------|------|----------|-----|
"""
        for e in fastest:
            net = e.net_type or "-"
            loc = e.location or "-"
            isp = (e.isp[:25] + "...") if e.isp and len(e.isp) > 25 else (e.isp or "-")
            md += f"| `{e.address}` | {e.latency_ms:.0f}ms | {net} | {loc} | {isp} |\n"
        
        md += """
---
*Auto-generated by IP Aggregation System v4.0*
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md)
    
    def _export_root_txt(self, entries: List[IPEntry]):
        """æ ¹ç›®å½•ç®€æ´æ ¼å¼"""
        filepath = "all.txt"
        
        lines = [
            f"# IP List - {self.timestamp}",
            f"# Total: {len(entries)}",
            ""
        ]
        lines.extend([e.address for e in entries])
        lines.append("")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))


# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    logger.info("=" * 60)
    logger.info("ğŸš€ IP Aggregation System v4.0")
    logger.info("=" * 60)
    logger.info(f"âš™ï¸  Validation: {'SKIP' if SKIP_VALIDATION else 'ENABLED'}")
    if not SKIP_VALIDATION:
        logger.info(f"âš™ï¸  Timeout: {VALIDATION_TIMEOUT}s | Concurrency: {VALIDATION_CONCURRENCY}")
    logger.info("=" * 60)
    
    # ===== é˜¶æ®µ 1: æ•°æ®é‡‡é›† =====
    logger.info("\nğŸ“¡ PHASE 1: Data Collection")
    logger.info("-" * 40)
    
    all_entries: List[IPEntry] = []
    source_stats: Dict[str, int] = {}
    
    for source in SOURCES:
        try:
            entries = process_source(source)
            all_entries.extend(entries)
            source_stats[source['name']] = len(entries)
        except Exception as e:
            logger.error(f"âŒ Error processing {source['name']}: {e}")
            source_stats[source['name']] = 0
    
    logger.info(f"\nğŸ“Š Raw total: {len(all_entries)}")
    
    # ===== é˜¶æ®µ 2: å»é‡æ’åº =====
    logger.info("\nğŸ”„ PHASE 2: Deduplication")
    logger.info("-" * 40)
    
    unique_entries = deduplicate_entries(all_entries)
    unique_entries = sort_entries(unique_entries)
    
    logger.info(f"ğŸ“Š Unique entries: {len(unique_entries)}")
    
    # ===== é˜¶æ®µ 3: éªŒè¯ =====
    if not SKIP_VALIDATION and unique_entries:
        logger.info("\nğŸ” PHASE 3: Validation")
        logger.info("-" * 40)
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(
                validate_entries_async(
                    unique_entries,
                    timeout=VALIDATION_TIMEOUT,
                    concurrency=VALIDATION_CONCURRENCY
                )
            )
        finally:
            loop.close()
        
        valid_count = sum(1 for e in unique_entries if e.is_valid)
        logger.info(f"\nğŸ“Š Valid: {valid_count}/{len(unique_entries)}")
    else:
        logger.info("\nâ­ï¸  PHASE 3: Validation SKIPPED")
    
    # ===== é˜¶æ®µ 4: å¯¼å‡º =====
    logger.info("\nğŸ’¾ PHASE 4: Export")
    logger.info("-" * 40)
    
    stats = {'sources': source_stats}
    exporter = Exporter(OUTPUT_DIR)
    exporter.export_all(unique_entries, stats)
    
    # ===== å®Œæˆ =====
    elapsed = time.time() - start_time
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ¨ COMPLETED")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Total: {len(unique_entries)} entries")
    logger.info(f"â±ï¸  Time: {elapsed:.1f}s")
    logger.info("=" * 60)
    
    # è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
    logger.info("\nğŸ“ Output files:")
    for f in ["all.txt", "all.json", "all.csv", "valid_only.txt", "summary.md"]:
        logger.info(f"   - output/{f}")
    logger.info("   - all.txt (root)")


if __name__ == "__main__":
    main()
